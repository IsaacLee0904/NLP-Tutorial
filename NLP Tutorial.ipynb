{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUMMARY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLP Fundamentals\n",
    "* Bag-of-Words\n",
    "* Word Vectors\n",
    "##### Misc NLP Techniques\n",
    "* Regexes \n",
    "* Stemming & Lemmatization\n",
    "#### State of the Art\n",
    "* Recurrent Neural Nets\n",
    "* Attention is all you need & Transformer Architecture\n",
    "* OpenAI GPT, BERT etc...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up \n",
    "### Install necessary libraries & download models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import svm\n",
    "import spacy\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some training utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Category:\n",
    "  BOOKS = \"BOOKS\"\n",
    "  CLOTHING = \"CLOTHING\"\n",
    "\n",
    "train_x = [\"i love the book\", \"this is a great book\", \"the fit is great\", \"i love the shoes\"]\n",
    "train_y = [Category.BOOKS, Category.BOOKS, Category.CLOTHING, Category.CLOTHING] # classify whether these are book category or cloth category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Word Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Concept：Fit vectorizer to transform text to bag-of-words vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['book' 'fit' 'great' 'is' 'love' 'shoes' 'the' 'this']\n",
      "[[1 0 0 0 1 0 1 0]\n",
      " [1 0 1 1 0 0 0 1]\n",
      " [0 1 1 1 0 0 1 0]\n",
      " [0 0 0 0 1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(binary = True)\n",
    "train_x_vectors = vectorizer.fit_transform(train_x)\n",
    "\n",
    "print(vectorizer.get_feature_names_out()) # 會有哪些字被切割出來\n",
    "print(train_x_vectors.toarray()) # 上面被切出來的字是否有出現在這個字串裡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build Train SVM Model\n",
    "clf_svm = svm.SVC(kernel = 'linear')\n",
    "clf_svm.fit(train_x_vectors, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BOOKS'], dtype='<U8')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test clf_svm\n",
    "test_x = vectorizer.transform(['i like the book']) # input the str want to predict\n",
    "\n",
    "clf_svm.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* limitatin：Not able to handle with the word not in the train data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors / Word Embedding\n",
    "Capture the semantic meaning of a word in a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i love the book', 'this is a great book', 'the fit is great', 'i love the shoes']\n"
     ]
    }
   ],
   "source": [
    "print(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take all the individual word embeddings and average them together\n",
    "docs = [nlp(text) for text in train_x] # turn train_x to [i love the book, this is a great book, the fit is great, i love the shoes]\n",
    "train_x_word_vectors = [x.vector for x in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build Train SVM Model\n",
    "clf_svm_wv = svm.SVC(kernel='linear')\n",
    "clf_svm_wv.fit(train_x_word_vectors, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BOOKS', 'BOOKS'], dtype='<U8')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test clf_svm_wv\n",
    "test_x = [\"I went to the bank and wrote a check\", \"let me check that out\"]\n",
    "test_docs = [nlp(text) for text in test_x]\n",
    "test_x_word_vectors =  [x.vector for x in test_docs]\n",
    "\n",
    "clf_svm_wv.predict(test_x_word_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regexes / Regular Expression\n",
    "Pattern matching of strings in Python. Ex. 123-123-1234 and 555-555-5555 ... are all phone number and we can use Regexes to identify them.\n",
    "\n",
    "Password checkers, phone number, email and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I liked that story.']\n"
     ]
    }
   ],
   "source": [
    "regexp = re.compile(r\"\\bread\\b|\\bstory\\b|book\")\n",
    "\n",
    "phrases = [\"I liked that story.\", \"the car treaded up the hill\", \"this hat is nice\"]\n",
    "\n",
    "matches = []\n",
    "for phrase in phrases:\n",
    "  if re.search(regexp, phrase):\n",
    "    matches.append(phrase)\n",
    "\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming / Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'read the book'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "phrase = \"reading the books\"\n",
    "words = word_tokenize(phrase)\n",
    "\n",
    "stemmed_words = []\n",
    "for word in words:\n",
    "  stemmed_words.append(stemmer.stem(word))\n",
    "\n",
    "\" \".join(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'read the book'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "phrase = \"reading the books\"\n",
    "words = word_tokenize(phrase)\n",
    "\n",
    "lemmatized_words = []\n",
    "for word in words:\n",
    "  lemmatized_words.append(lemmatizer.lemmatize(word, pos='v'))\n",
    "\n",
    "\" \".join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords\n",
    "A set of most common words in language and need to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here example sentence demonstrating removal stopwords'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english') # set up the common words English\n",
    "\n",
    "phrase = \"Here is an example sentence demonstrating the removal of stopwords\"\n",
    "\n",
    "words = word_tokenize(phrase)\n",
    "\n",
    "stripped_phrase = []\n",
    "for word in words:\n",
    "  if word not in stop_words:\n",
    "    stripped_phrase.append(word)\n",
    "\n",
    "\" \".join(stripped_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Various other techniques (spell correction, sentiment, & pos tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "phrase = \"the book was horrible\"\n",
    "\n",
    "tb_phrase = TextBlob(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"the book was horrible\")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cant use to correct the spelling\n",
    "# if phrase =  \"the book was horribler\" could get the same output\n",
    "tb_phrase.correct() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 'DT'), ('book', 'NN'), ('was', 'VBD'), ('horrible', 'JJ')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word tags\n",
    "tb_phrase.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=-1.0, subjectivity=1.0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentiment analysis\n",
    "tb_phrase.sentiment"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6602c2885c02d123719f05763fdd31327dda850adf85d09c83d4eb769f341e41"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
